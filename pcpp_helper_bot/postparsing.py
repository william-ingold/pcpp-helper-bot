import re

from bs4 import BeautifulSoup
from pcpp.pcppparser import PCPPParser

# Set of Regular Expressions used for searching posts
anon_list_pat = r'https://(\w+\.)?pcpartpicker\.com/list/\w+'
identifiable_list_pat = r'https://(\w+\.)?pcpartpicker\.com/user/.+'
anon_re = re.compile(anon_list_pat)
iden_re = re.compile(identifiable_list_pat)
footer_re = re.compile(r'(Total)|(Generated by)')

# So we can get anon urls from identifiable urls
pcpp_parser = PCPPParser()


class Table:
    def __init__(self, pcpp_url: str, head: BeautifulSoup, foot: BeautifulSoup):
        self.pcpp_url = pcpp_url
        self.thead = head
        self.tfoot = foot
    
    def is_valid(self):
        return self.thead is not None and self.tfoot is not None


def pcpp_table_header(tag: BeautifulSoup):
    """Find table headers that match one generated by PCPP."""
    return tag.name == 'thead' and tag.text.strip() == 'Type\nItem\nPrice'


def detect_escaped_markdown(markdown):
    # \[PCPartPicker Part List\] Link at head or foot
    bad_link_pat = r'\\\[PCPartPicker( Part List)?\\\]'
    
    # \| -> Table cell divider
    bad_pipe_pat = r'\\\|'
    
    bad_link = re.search(bad_link_pat, markdown)
    bad_pipes = re.findall(bad_pipe_pat, markdown)
    
    return bad_link or len(bad_pipes) >= 2


def detect_pcpp_html_elements(text: str):
    """Detects PCPP HTML elements in the provided text.
    
    Args:
        text (str): HTML text to search.
    Returns:
        Dictionary of lists with the following keys: anon, identifiable,
        pcpp_headers,  table_headers, table_footers.
    """
    
    soup = BeautifulSoup(text, 'lxml')
    
    # Search for all the PCPP links
    anon_links = soup.find_all('a', href=anon_re)
    iden_links = soup.find_all('a', href=iden_re)
    
    anon_urls = []
    iden_urls = []
    
    # Grab the urls only
    if anon_links and len(anon_links) != 0:
        anon_urls = [a['href'] for a in anon_links]
        
    if iden_links and len(iden_links) != 0:
        iden_urls = [a['href'].replace('#view=', '') for a in iden_links]
    
    table_headers = soup.find_all(pcpp_table_header)
    pcpp_tables = []
    
    # Look for appropriate table headers. Look for a
    # PCPP anon link before the table and also look for
    # a valid/expected table footer.
    for table_header in table_headers:
        list_url = table_header.find_previous('a', href=anon_re)
        if list_url:
            list_url = list_url['href']
        
        footer = table_header.find_next('td', text=footer_re)
        pcpp_tables.append(Table(list_url, table_header, footer))
    
    return {'anon': anon_urls, 'identifiable': iden_urls,
            'tables': pcpp_tables}


def get_urls_with_no_table(urls: list, tables: list):
    """Determines what urls do not have an accompanying table.
    
    Args:
        urls (list): A list of PCPP urls.
        tables (list): A list of table elements found in the post.
    
    Returns:
        List of urls that LIKELY have no accompanied table.
    """
    
    unpaired_tables = []
    
    # Remove urls that are already paired with a table.
    # Track valid tables not paired with a url.
    for table in tables:
        if table.is_valid():
            if table.pcpp_url:
                urls.remove(table.pcpp_url)
            else:
                unpaired_tables.append(table)
    
    # TODO: COULD scrape the urls and compare to tables... but seems overkill
    # If there are urls left and unpaired tables left then just assume
    # that the first X urls unpaired go with the X unpaired tables
    if len(urls) >= len(unpaired_tables):
        for i in range(len(unpaired_tables)):
            urls.pop(0)
            
    return urls


def combine_iden_anon_urls(anon_urls: list, iden_urls: list):
    """Gets the anonymous list urls for identifiable urls and combines them
    with the anonymous urls.
    
    Args:
        anon_urls (list): List of anonymous list urls.
        iden_urls (list): List of identifiable list urls.
        
    Returns:
        A tuple of previous anonymous urls along with the anonymous urls of the
        identifiable list urls (if it wasn't already in the list) and a
        list of identifiable urls with their anonymous versions.
    """
    
    new_anon_urls = [(url, pcpp_parser.get_anon_list_url(url)) for url in iden_urls]
    anon_urls += [anon for _, anon in new_anon_urls if anon not in anon_urls]
    
    return anon_urls, new_anon_urls


def count_table_types(tables):
    table_count = len(tables)
    valid_tables = 0
    invalid_tables = 0
    
    if table_count != 0:
        for table in tables:
            if table.is_valid():
                valid_tables += 1
            else:
                invalid_tables += 1

    return table_count, valid_tables, invalid_tables
    
    
def parse_submission(submission_html, submission_markdown):
    rem_pcpp_urls = []
    table_data = {}
    
    # Parse the submission's post
    pcpp_elements = detect_pcpp_html_elements(submission_html)
    found_bad_markdown = detect_escaped_markdown(submission_markdown)

    # Convert identifiable links to anonymous links
    # Combine with the anonymous too
    all_pcpp_urls, iden_anon_list = combine_iden_anon_urls(pcpp_elements['anon'],
                                                           pcpp_elements['identifiable'])

    # If we found any list urls
    if len(all_pcpp_urls) != 0:
        # Tables can be non-existent, valid, or invalid
        total_tables, valid_tables, invalid_tables = count_table_types(pcpp_elements['tables'])
        
        table_data = {'total': total_tables, 'valid': valid_tables,
                      'invalid': invalid_tables, 'bad_markdown': found_bad_markdown}
    
        # Find what URLs don't have an accompanying table
        rem_pcpp_urls = get_urls_with_no_table(all_pcpp_urls,
                                               pcpp_elements['tables'])
    
    return rem_pcpp_urls, iden_anon_list, table_data


